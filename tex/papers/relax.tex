\section{RELAX}
In this section we review \citet{grathwohl2017relax}.

\paragraph{Motivation}
The aim of this paper is to find $\argmin\theta \Es{p(b \mid \theta)}{f(b)}$
via gradient descent using a Monte Carlo approximation of the gradient.
The focus is finding a low-variance unbiased estimator by combining
the score function estimator with the reparameterization trick.

The two applications considered in this paper are gradient
estimation for latent variable models and RL.

\paragraph{Contributions}
\citet{grathwohl2017relax} introduce a differentiable surrogate for $f$, and for discrete $b$
a reparameterizable relaxation $p_\theta(z)$ such that $H(z) = b$.
They use the differentiable surrogate (and $p_\theta(z)$ when applicable) as a control
variate and directly minimize the variance of the score function estimator.
The result is an unbiased and seemingly low-variance estimator.

Additionally, for discrete $b$,
they resample the relaxed $\tilde{z} \mid b$,
which allows them to further reduce variance by increasing the correlation between
their control variate and the score function estimator.

More concretely, let $b \sim \Cat(\theta), \theta \in \Delta^{n-1}$.
The score function estimator is
\begin{equation}
\nabla_\theta \Es{b}{f(b)} = \Es{b}{f(b)\nabla_\theta \log p(b\mid\theta)}
\end{equation}

The DLAX estimator is given by introducing differentiable surrogate $c_\phi(z)$
with Gumbel-perturbed logits $z = T(\theta, \epsilon), \epsilon\sim\Gumbel(0,1)$.
\begin{equation}
\nabla_\theta \Es{b}{f(b)} = \Es{\epsilon}{f(b)\nabla_\theta \log p(b\mid\theta)
- c_\phi(z)\nabla_\theta \log p(z \mid \theta)
+ \underbrace{\nabla_\theta c_\phi(z)}_\textrm{Reparam}}
\end{equation}
where $b = H(z), z=T(\theta,\epsilon),\epsilon\sim \Gumbel(0,1)$ and $H$ is a deterministic transformation.
For the case of $z\sim\Gumbel(\theta)$, $H(z) = \argmax_i(z_i)$.

The RELAX estimator is obtained by further resampling $\tilde{z}\mid b$.
With some rewriting, we obtain
\begin{equation}
\nabla_\theta \Es{b}{f(b)}
= \Es{\epsilon}{(f(b) - \Es{\tilde{z}\mid b}{c_\phi(\tilde{z})})\nabla_\theta \log p(b\mid\theta)
- \nabla_\theta\Es{\tilde{z}\mid b}{ c_\phi(\tilde{z})} + \nabla_\theta c_\phi(z)}
\end{equation}
See Appendix A for the derivation.
The second to last term is of interest, as it entails resampling $\tilde{z} \mid b$.
This term is also assumed to be reparameterizable.
In total, the RELAX estimator requires one evaluation of $f$
at $b$ and two evaluations of $c_\phi$ at $z,\tilde{z}$.

In summary:
\begin{enumerate}
\item Introduce differentiable surrogate $c_\phi$ as reparameterizable control variate 
\item Condition on $b$ to increase correlation of control variate with estimand
\end{enumerate}

\paragraph{Results}
Show better training and validation performance on a mixture density model for MNIST
Better training but overfits on omniglot
Demonstrate faster convergence and lower variance gradient estimators than
A2C for three RL tasks: cart-pole, lunar lander, and inverted pendulum

\paragraph{Limitations}
\begin{itemize}
\item No empirical validation of variance reduction in DLAX to RELAX
\item Likely requires combination with other variance reduction techniques when dealing with
    large, unstructured distributions (unavoidable).
\end{itemize}

\paragraph{Questions / Comments}
\begin{itemize}
\item I would have liked to see experiments analyzing the effect of parameter sharing between
$f$ and $c_\theta$ when possible.
\end{itemize}

